


# Imports
import pandas as pd
from pathlib import Path
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder








# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame
applicants_data_df = pd.read_csv(
    Path('Resources/applicants_data.csv')
)

# Review the DataFrame
applicants_data_df.head()


# Review the data types associated with the columns
applicants_data_df.dtypes





# Drop the 'EIN' and 'NAME' columns from the DataFrame
applicants_data_df = applicants_data_df.drop(columns =['EIN','NAME'])

# Review the DataFrame
applicants_data_df.head()


applicants_data_df.dtypes





# Create a list of categorical variables 
categorical_variables = list(applicants_data_df.dtypes[applicants_data_df.dtypes == 'object'].index)

# Display the categorical variables list
categorical_variables


# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False)


# Encode the categorcal variables using OneHotEncoder
encoded_data = enc.fit_transform(applicants_data_df[categorical_variables])
encoded_data


# Create a DataFrame with the encoded variables
encoded_df = pd.DataFrame(
    encoded_data,
    columns = enc.get_feature_names_out(categorical_variables)
)

# Review the DataFrame
encoded_df.head()





numerical_variables_df = applicants_data_df.drop(columns = categorical_variables)
numerical_variables_df.head()


# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame
applicants_df = pd.concat(
    [ numerical_variables_df,
      encoded_df
    ],
    axis=1
)

# Review the Dataframe
applicants_df.head()


column_names = applicants_df.columns
print(column_names)





# Define the target set y using the IS_SUCCESSFUL column
y = applicants_df['IS_SUCCESSFUL']

# Display a sample of y
y.head()


# Define features set X by selecting all columns but IS_SUCCESSFUL
X = applicants_df.drop(columns=['IS_SUCCESSFUL'])

# Review the features DataFrame
X.head()





# Split the preprocessed data into a training and testing dataset
# Assign the function a random_state equal to 1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)





# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the scaler to the features training dataset
X_scaler = scaler.fit(X_train)

# Fit the scaler to the features training dataset
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)








# Define the the number of inputs (features) to the model
number_input_features = len(X_train.columns)

# Review the number of features
number_input_features


# Define the number of neurons in the output layer
number_output_neurons = 1


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1 =  60
# Review the number hidden nodes in the first layer
hidden_nodes_layer1


# Define the number of hidden nodes for the second hidden layer
hidden_nodes_layer2 = 30

# Review the number hidden nodes in the second layer
hidden_nodes_layer2


# Create the Sequential model instance
nn_A0 = Sequential()


# Add the first hidden layer
nn_A0.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation='relu'))


# Add the second hidden layer
nn_A0.add(Dense(units=hidden_nodes_layer2, activation='relu'))


# Add the output layer to the model specifying the number of output neurons and activation function
nn_A0.add(Dense(units=number_output_neurons, activation='sigmoid'))


# Display the Sequential model summary
nn_A0.summary()





# Compile the Sequential model
nn_A0.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


# Fit the model using 20 epochs and the training data
number_of_epochs = 20
nn_A0.fit(X_train_scaled, y_train, 
                    epochs= number_of_epochs,
                    batch_size= 80,
                    shuffle=True)





# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A0.evaluate(X_test_scaled, y_test, verbose = 2)
# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")





# Set the model's file path
file_path = 'AlphabetSoup.h5'

# Export your model to a HDF5 file
nn_A0.save(file_path)











# Define the the number of inputs (features) to the model
number_input_features = len(X_train.iloc[0])
# Review the number of features
number_input_features


# Define the number of neurons in the output layer
number_output_neurons_A1 = 1


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1_A1 = 56
# Review the number of hidden nodes in the first layer
hidden_nodes_layer1_A1


# Define the number of hidden nodes for the second hidden layer
hidden_nodes_layer2_A1 = 28

# Review the number hidden nodes in the second layer
hidden_nodes_layer2_A1


# Create the Sequential model instance
nn_A1 = Sequential()


# Import the `Dropout` layer
from keras.layers import Dropout
# Import regularizers
from keras.regularizers import l1, l2


# Add the first hidden layer
nn_A1.add(Dense(units=hidden_nodes_layer1_A1, input_dim=number_input_features, activation='relu'))

# Now add a dropout layer with dropout rate of 0.2
nn_A1.add(Dropout(0.2))

# Adding regularization to second hidden layer
nn_A1.add(Dense(units=hidden_nodes_layer2_A1, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))

# Add the output layer to the model specifying the number of output neurons and activation function
nn_A1.add(Dense(units=number_output_neurons_A1, activation='sigmoid'))


# Check the structure of the model
nn_A1.summary()


# Compile the Sequential model
nn_A1.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Fitting the Alternative Model 1 
number_of_epochs = 50
nn_A1.fit(X_train_scaled,y_train, 
                    epochs=number_of_epochs,
                    batch_size= 100,
                    shuffle=True)


# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A1.evaluate(X_test_scaled, y_test, verbose=2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")





# Define the the number of inputs (features) to the model
number_input_features = len(X_train.iloc[0])

# Review the number of features
number_input_features


# Define the number of neurons in the output layer
number_output_neurons_A2 = 1


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1_A2 = 50

# Review the number of hidden nodes in the first layer
hidden_nodes_layer1_A2


# Define the number of hidden nodes for the second hidden layer
hidden_nodes_layer2_A2 = 25

# Review the number hidden nodes in the second layer
hidden_nodes_layer2_A2


# Create the Sequential model instance
nn_A2 = Sequential()


# Add the first hidden layer
nn_A2.add(Dense(units=hidden_nodes_layer1_A2, input_dim=number_input_features, activation='relu'))

# Now add a dropout layer with dropout rate of 0.4
nn_A2.add(Dropout(0.4))

#Adding regularization to second hidden layer
nn_A2.add(Dense(units=hidden_nodes_layer2_A2, activation='relu'))

# Add the output layer to the model specifying the number of output neurons and activation function
nn_A2.add(Dense(units=number_output_neurons_A1, activation='sigmoid'))

# Check the structure of the model
nn_A2.summary()


# Compile the model
nn_A2.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Fit the model
number_of_epochs = 90
nn_A2.fit(X_train_scaled,y_train, 
                    epochs=number_of_epochs,
                    batch_size= 120,
                    shuffle=True)





print("Original Model Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A0.evaluate(X_test_scaled, y_test, verbose =2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


print("Alternative Model 1 Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A1.evaluate(X_test_scaled, y_test, verbose = 2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


print("Alternative Model 2 Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A2.evaluate(X_test_scaled, y_test, verbose =2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")





# Set the file path for the first alternative model
file_path = 'AlphabetSoup_A1.h5'

# Export your model to a HDF5 file
nn_A1.save(file_path)


# Set the file path for the second alternative model
file_path = 'AlphabetSoup_A2.h5'

# Export your model to a HDF5 file
nn_A2.save(file_path)



